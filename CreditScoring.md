Credit Application Scoring
==========================

Within the scoring field there are 'application' and 'behavioural' scoring models.

Application scoring is for credit applicants. It uses information submitted to lenders by applicants themselves, and information from debt bureaus, to produce a one-off, point of application estimate of default likelihood over a given time period. Behavioural scoring is for customers. It uses the account information generated by ongoing customer behaviour to forecast future behaviour. Credit has already been granted, the lender is committed for the duration, so behavioural scoring is used to forecast default risk and thereby manage the exposure (Bijak & Thomas, 2012; Kennedy et al., 2013). Only application scoring is relevant for this research. The experimental data consists exclusively of data available at the point of application and predictions consist of one-off estimates of default risk.

The appropriate statistical framework is 'classification'. This is an old modelling paradigm with a mature research literature. Approaches to binary classification (e.g. 1 vs. 0, default vs. not default) are legion within the credit scoring discipline. Recent literature reviews (Crook, Edelman, & Thomas, 2007; Crook, Thomas, & Edelman, 2012) show that potential application scoring options include Neural Networks, *k*-Nearest Neighbour clustering, Genetic Algorithms, Support Vector Machines (SVM), and ensemble methods such as Random Forests (RF) and Gradient Boosting Machines (GBM). This range can be expanded with bespoke combinations of the above into hybrid classification methods that aggregate predictions from different types of models (Reddy & Ravi, 2013). All algorithms have fine tuning parameters that alter performance.

These advanced classifiers usually outperform traditional approaches in a laboratory environment. However, it does not follow that they are therefore part of the branch-level industry toolkit. It is challenging for in-house scorecard developers to replicate performance established in a laboratory on propriety market-sourced data sets. Consider SVM as an example. It has been long noted as a suitable contender for business application. However, it has high data-quality requirements that rarely present in research, and which can prevent it from making meaningful improvements to predictive accuracy when applied to real propriety data (Crook et al., 2007).

Recent research does not support that SVM will even produce superior models compared to those it is meant to supersede. When data sets are large and balanced, meaning they have sufficient defaulting records, traditional Logistic Regression (LOG) models remain competitive (Musa, 2013). Brown and Mues (2012) show that SVM performs poorly against LOG and another traditional classifier, Linear Discriminant Analysis (DA), with both LOG and DA being competitive against all modern alternatives save recent ensemble algorithms.

These benchmarking results do not provide a compelling rationale for lenders to dismantle existing institutionalised modelling systems and adopt modern alternatives. The transition would be costly and deliver limited performance gains, especially for lenders possessing large data sets that can employ traditional classifiers (e.g. LOG) competitively.

For model comparisons to be generalised to branch practice the classification model must resemble what is presently in use. Of the above classifiers, several textbook references assert that LOG is the backbone of industry application scoring (Anderson, 2007, p.58; Siddiqi, 2006, p.89-90; Thomas, Edelman, & Crook, 2002, p.41). After decades of industry acceptance, "logistic regression and linear programming" (i.e. mathematical optimisation) are the "two main stalwarts of today's card builders" (Thomas, 2000, p.152). LOG is almost universally included in benchmarking research of modern classifiers as the industry standard against which alternatives are judged.

LOG can model relationships between a dichotomous response variable, typically coded in binary form (e.g. 1 vs. 0, default vs. not default), and one or more regressors. A reason for the popularity of LOG in classification problems is the logistic function, *f*(*z*) = 1 / (1+*e*<sup>-*z*</sup>), where -Inf *GE* *z* *GE* Inf and 0 *GE* *f*(*z*) *GE* 1. This alters parameter estimation as compared to Linear Regression (LR) and allows LOG to output likelihood predictions as a probability of the modelled event occurring (Kleinbaum & Klein, 2010, p.5).

LR estimates parameters arithmetically by minimising the sum-of-squared deviations between predicted and actual values (i.e. least squares). In contrast, LOG transforms regressors into a logistic function and iteratively solves for the best parameters using Maximum Likelihood Estimation (MLE). Parameters are 'guessed' (i.e. starting values) then iteratively adjusted to maximise log likelihood until the method converges on an optimal solution. As Hox (2010) summarises, the estimates produced by MLE "are those parameter estimates that maximize the probability of finding the sample data that we have actually found" (p.16).

There are three main reasons LOG overtook the LR alternative and came to dominate industry scorecard development. The first is the noted facility to output robust and accurate probabilities for a dichotomous response variable (Thomas, Edelman, & Crook, 2002, p.65). The second is that LOG has less restrictive assumptions. Because the predicted values are binary, 'linear probability modelling' (i.e. using LR for classification) violates the assumption of normally distributed and homoscedastic residuals. It is also prone to outputting impossible probability values (e.g. negatives). In contrast, LOG has no assumptions about regressor distributions and cannot output probabilities beyond the 0 to 1 range (Tabachnick & Fidell, 2013, p.439). LOG only requires that relationships be linear within the logistic function, that residuals be independent, and the absence of multicollinearity (i.e. where regressors are related by a linear function; Everitt & Skrondal, 2010, p.287). In practice, even these LOG assumptions can be violated, yet LOG has still emerged as a suitable option, or at least, a 'less unsuitable' one.

The third reason LOG usage grew was advances in computer processing power (Crook, Thomas, & Edelman, 2012). The MLE used to estimate LOG parameters avoids some assumptions, restrictions and biases, at the cost of greater computing time. According to Anderson (2007), this was "infeasible at a time when computers were big and slow", but today differences in computational time are "hardly noticeable", and consequently LOG "is used by between 80 and 90 per cent of scorecard developers" (p.165). LOG is a relatively fast model to train on current hardware, especially given that leading alternatives are computationally intensive machine-learners. Eventually those alternatives may reduce the need for reject inference altogether (e.g. 'Transductive' SVM; Maldonado & Paredes, 2010). The steep trajectory of computing power is informative as to how the branch-level toolkit may evolve. Whatever future industry preferences may be, for the time being, LOG is the unchallenged industry standard.

References
==========

Anderson, R. (2007). The Credit Scoring Toolkit: Theory and Practice for Retail Credit Risk Management and Decision Automation. New York, USA: Oxford University Press.

Bijak, K., Thomas, L.C. (2012). Does segmentation always improve model performance in credit scoring? Expert Systems with Applications, 39, 2433-2442.

Crook, J., Edelman, D.B., Thomas, L.C. (2007). Recent developments in consumer credit risk assessment. European Journal of Operational Research, 183, 1447-1465.

Crook, J., Thomas, L., Edelman, D. (2012). Editorial. International Journal of Forecasting, 28, 128-132.

Everitt, B.S., Skrondal, A.S. (2010). The Cambridge Dictionary of Statistics (4th ed.). Cambridge, UK: Cambridge University Press.

Hox, J. (2010). Multilevel Analysis: Techniques and Applications (2nd ed.). Great Britain, UK: Routledge.

Kennedy, K., Mac Namee, B., Delany, S.J., O'Sullivan, M., Watson, N. (2013). A window of opportunity: Assessing behavioural scoring. Expert Systems with Applications, 40, 1372-1380.

Kleinbaum, D.G, Klein, M. (2010). Logistic Regression: A Self-Learning Text (3rd ed.). New York, USA: Springer.

Maldonado, S., Paredes, G. (2010). A Semi-supervised Approach for Reject Inference in Credit Scoring Using SVMs Advances in Data Mining. Applications and Theoretical Aspects Lecture Notes in Computer Science Volume, 6171, 558-571.

Musa, A.B. (2013). Comparative study on classification performance between support vector machine and logistic regression. International Journal of Machine Learning and Cybernetics, 4, 13-24.

Reddy, K.N., Ravi, V. (2013). Differential evolution trained kernel principal component WNN and kernel binary quantile regression: Application to banking. Knowledge-Based Systems, 39, 45-56.

Siddiqi, N. (2006). Credit Risk Scorecards: Developing and Implementing Intelligent Credit Scoring. New Jersey, USA: John Wiley & Sons, Inc.

Tabachnick, B.G., Fidell, L.S. (2013). Using Multivariate Statistics (6th ed.). USA: Pearson Education, Inc.

Thomas, L.C. (2000). A survey of credit and behavioural scoring: forecasting financial risk of lending to consumers. International Journal of Forecasting, 16, 149-172.

Thomas, L.C., Edelman, D.B., Crook, J.N. (2002). Credit Scoring and its Applications. Philadelphia, USA: SIAM.
